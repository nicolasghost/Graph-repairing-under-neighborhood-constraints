{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ffe08d4",
   "metadata": {},
   "source": [
    "# Cora Co-author Graph Import & Perturbation\n",
    "\n",
    "This notebook:\n",
    "1. Loads author co-authorship data from the Cora dataset\n",
    "2. Imports clean data into a **constraint graph** (canonical reference)\n",
    "3. Creates an **instance graph** (copy with intentional perturbations)\n",
    "4. Swaps random author identities to simulate data quality issues\n",
    "\n",
    "**Workflow:**\n",
    "- Environment setup → Load data → Import to Neo4j → Create perturbations → Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4fb616",
   "metadata": {},
   "source": [
    "## Step 1: Environment Configuration\n",
    "\n",
    "Load Neo4j credentials and database names from `.env` file.\n",
    "Never commit secrets — use `.env` (gitignored) for local credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c855f969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Neo4j URI: 'neo4j://127.0.0.1:7687', username: 'neo4j' (password hidden)\n"
     ]
    }
   ],
   "source": [
    "# --- Env loader & sanitiser ---\n",
    "import os\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "# .env path \n",
    "env_path = pathlib.Path.cwd() / \".env\"\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "def _strip_quotes(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    return v.strip().strip('\"').strip(\"'\")\n",
    "\n",
    "# sanitized env values\n",
    "URI = _strip_quotes(os.getenv(\"NEO4J_URI\"))\n",
    "USERNAME = _strip_quotes(os.getenv(\"NEO4J_USERNAME\"))\n",
    "PASSWORD = _strip_quotes(os.getenv(\"NEO4J_PASSWORD\"))\n",
    "\n",
    "missing = [k for k,v in ((\"NEO4J_URI\",URI),(\"NEO4J_USERNAME\",USERNAME),(\"NEO4J_PASSWORD\",PASSWORD)) if not v]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing env vars: {missing}. Edit .env or export env vars and re-run this cell.\")\n",
    "\n",
    "# printing safe info only\n",
    "print(f\"Loaded Neo4j URI: {URI!r}, username: {USERNAME!r} (password hidden)\")\n",
    "AUTH = (USERNAME, PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d33d7",
   "metadata": {},
   "source": [
    "## Step 2: Configure Databases & Parameters\n",
    "\n",
    "Set constraint and instance database names, fraud count.\n",
    "All values can be overridden via `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "402aa3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint DB: coauthor-constraint\n",
      "Instance DB: coauthor-instance\n",
      "Fraud / swap count: 10\n"
     ]
    }
   ],
   "source": [
    "# --- DB names, hyperparameters, and defaults ---\n",
    "CONSTRAINT_DB = _strip_quotes(os.getenv(\"NEO4J_CONSTRAINT_DB\")) or \"test\"\n",
    "INSTANCE_DB   = _strip_quotes(os.getenv(\"NEO4J_INSTANCE_DB\")) or \"test-instance-graph\"\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "hypp = edict()\n",
    "hypp.fraud_number = int(os.getenv(\"FRAUD_NUMBER\") or 10)\n",
    "\n",
    "print(\"Constraint DB:\", CONSTRAINT_DB)\n",
    "print(\"Instance DB:\", INSTANCE_DB)\n",
    "print(\"Fraud / swap count:\", hypp.fraud_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804cef1",
   "metadata": {},
   "source": [
    "## Step 3: Locate Data Files\n",
    "\n",
    "Auto-detects the latest timestamped author/connection files from `datasets/temp/`.\n",
    "These are generated by `cora_data_cleaning.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c021f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AUTHORS_PATH = datasets\\temp\\authors_20260107-111333.txt\n",
      "Using CONNECTIONS_PATH = datasets\\temp\\author_connections_20260107-111333.txt\n"
     ]
    }
   ],
   "source": [
    "# --- Pick latest generated authors/connection files automatically ---\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def latest_by_timestamp(folder: Path, pattern: str, name_re: str, dt_fmt=\"%Y%m%d-%H%M%S\"):\n",
    "    folder = Path(folder)\n",
    "    candidates = list(folder.glob(pattern))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    ts_files = []\n",
    "    rx = re.compile(name_re)\n",
    "    for p in candidates:\n",
    "        m = rx.search(p.name)\n",
    "        if m:\n",
    "            try:\n",
    "                ts = datetime.strptime(m.group(1), dt_fmt)\n",
    "                ts_files.append((ts, p))\n",
    "            except Exception:\n",
    "                pass\n",
    "    if ts_files:\n",
    "        return str(max(ts_files, key=lambda t_p: t_p[0])[1])\n",
    "    return str(max(candidates, key=lambda p: p.stat().st_mtime))\n",
    "\n",
    "base = Path(\"datasets\") / \"temp\"\n",
    "AUTHORS_PATH = latest_by_timestamp(base, \"authors_*.txt\", r\"authors_(\\d{8}-\\d{6})\\.txt\")\n",
    "CONNECTIONS_PATH = latest_by_timestamp(base, \"author_connections_*.txt\", r\"author_connections_(\\d{8}-\\d{6})\\.txt\")\n",
    "\n",
    "if not AUTHORS_PATH or not CONNECTIONS_PATH:\n",
    "    raise FileNotFoundError(f\"No authors/connection files found in {base}. Run `cora_data_cleaning.ipynb` first.\")\n",
    "\n",
    "print(\"Using AUTHORS_PATH =\", AUTHORS_PATH)\n",
    "print(\"Using CONNECTIONS_PATH =\", CONNECTIONS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c170e3eb",
   "metadata": {},
   "source": [
    "## Step 4: Load Data into Memory\n",
    "\n",
    "Reads authors and co-author pairs from generated files.\n",
    "Parses connection format: `(author1,author2)` tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eefe20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 47 authors and 120 connections.\n"
     ]
    }
   ],
   "source": [
    "# --- Read authors and connection pairs ---\n",
    "# Read authors file\n",
    "with open(AUTHORS_PATH, encoding=\"utf-8\") as f:\n",
    "    authors = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Read connections file (expects lines like \"(authorA,authorB)\")\n",
    "connections = []\n",
    "with open(CONNECTIONS_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        clean_line = line.strip().lstrip(\"(\").rstrip(\")\")\n",
    "        parts = [p.strip() for p in clean_line.split(\",\")]\n",
    "        if len(parts) >= 2:\n",
    "            connections.append((parts[0], parts[1]))\n",
    "\n",
    "print(f\"Loaded {len(authors)} authors and {len(connections)} connections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afcc56d",
   "metadata": {},
   "source": [
    "## Step 5: Define Neo4j Operations\n",
    "\n",
    "Helper functions:\n",
    "- `clear_database()` — remove all nodes/relationships\n",
    "- `setup_database()` — create uniqueness constraints\n",
    "- `import_data()` — bulk insert authors and co-author edges\n",
    "- `swap_random_authors()` — swap names and label fraudulent nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c55f3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Neo4j helper functions and import/swap workflow ---\n",
    "from neo4j import GraphDatabase, basic_auth\n",
    " \n",
    "def clear_database(driver, database):\n",
    "    driver.execute_query(\"MATCH (n) DETACH DELETE n\", database_=database)\n",
    "    print(f\"Database '{database}' cleared.\")\n",
    "\n",
    "def setup_database(driver, database):\n",
    "    driver.execute_query(\"\"\"\n",
    "        CREATE CONSTRAINT author_name_unique IF NOT EXISTS\n",
    "        FOR (a:Author) REQUIRE a.name IS UNIQUE\n",
    "    \"\"\", database_=database)\n",
    "    print(f\"Constraint ensured on '{database}'.\")\n",
    "\n",
    "def import_data(driver, authors_list, connections_list, database):\n",
    "    # Bulk create authors\n",
    "    driver.execute_query(\"\"\"\n",
    "        UNWIND $names AS name\n",
    "        MERGE (:Author {name: name})\n",
    "    \"\"\", names=authors_list, database_=database)\n",
    "    # Bulk create connections\n",
    "    driver.execute_query(\"\"\"\n",
    "        UNWIND $pairs AS pair\n",
    "        MATCH (a:Author {name: pair[0]})\n",
    "        MATCH (b:Author {name: pair[1]})\n",
    "        MERGE (a)-[:CO_AUTHOR]->(b)\n",
    "    \"\"\", pairs=connections_list, database_=database)\n",
    "    print(f\"Imported data into '{database}' (authors: {len(authors_list)}, pairs: {len(connections_list)})\")\n",
    "\n",
    "def swap_random_authors(driver, database, number_of_swaps):\n",
    "    # Safer swap using a temporary property\n",
    "    query = \"\"\"\n",
    "    MATCH (a1:Author), (a2:Author)\n",
    "    WHERE id(a1) < id(a2)\n",
    "    WITH a1, a2 ORDER BY rand() LIMIT $limit\n",
    "    CALL {\n",
    "      WITH a1, a2\n",
    "      SET a1._tmp = a1.name\n",
    "      SET a1.name = a2.name\n",
    "      SET a2.name = a1._tmp\n",
    "      REMOVE a1._tmp\n",
    "      SET a1:Fraudulent, a2:Fraudulent\n",
    "      RETURN a1.name AS swapped_name1, a2.name AS swapped_name2\n",
    "    }\n",
    "    RETURN swapped_name1, swapped_name2\n",
    "    \"\"\"\n",
    "    with driver.session(database=database) as session:\n",
    "        result = session.run(query, limit=number_of_swaps)\n",
    "        swapped = []\n",
    "        for record in result:\n",
    "            swapped.append((record[\"swapped_name1\"], record[\"swapped_name2\"]))\n",
    "        print(f\"Performed {len(swapped)} swaps in '{database}'\")\n",
    "        return swapped   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c040ae",
   "metadata": {},
   "source": [
    "## Step 6: Execute Import & Perturbation Workflow\n",
    "\n",
    "Run the full pipeline:\n",
    "1. Verify Neo4j connectivity\n",
    "2. Import clean data to constraint DB\n",
    "3. Copy data to instance DB\n",
    "4. Swap random author pairs (simulate fraud)\n",
    "\n",
    "**Note:** Instance DB has NO uniqueness constraint (allows temporary duplicates during swap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83cf8011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Neo4j database successfully.\n",
      "Database 'coauthor-constraint' cleared.\n",
      "Constraint ensured on 'coauthor-constraint'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N00', status_description='warn: feature deprecated. CALL subquery without a variable scope clause is deprecated. Use CALL (a1, a2) { ... }', position=<SummaryInputPosition line=5, column=5, offset=111>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 111, 'line': 5, 'column': 5}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n    MATCH (a1:Author), (a2:Author)\\n    WHERE id(a1) < id(a2)\\n    WITH a1, a2 ORDER BY rand() LIMIT $limit\\n    CALL {\\n      WITH a1, a2\\n      SET a1._tmp = a1.name\\n      SET a1.name = a2.name\\n      SET a2.name = a1._tmp\\n      REMOVE a1._tmp\\n      SET a1:Fraudulent, a2:Fraudulent\\n      RETURN a1.name AS swapped_name1, a2.name AS swapped_name2\\n    }\\n    RETURN swapped_name1, swapped_name2\\n    '\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or consider using an application-generated id.', position=<SummaryInputPosition line=3, column=11, offset=46>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 46, 'line': 3, 'column': 11}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n    MATCH (a1:Author), (a2:Author)\\n    WHERE id(a1) < id(a2)\\n    WITH a1, a2 ORDER BY rand() LIMIT $limit\\n    CALL {\\n      WITH a1, a2\\n      SET a1._tmp = a1.name\\n      SET a1.name = a2.name\\n      SET a2.name = a1._tmp\\n      REMOVE a1._tmp\\n      SET a1:Fraudulent, a2:Fraudulent\\n      RETURN a1.name AS swapped_name1, a2.name AS swapped_name2\\n    }\\n    RETURN swapped_name1, swapped_name2\\n    '\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or consider using an application-generated id.', position=<SummaryInputPosition line=3, column=20, offset=55>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 55, 'line': 3, 'column': 20}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n    MATCH (a1:Author), (a2:Author)\\n    WHERE id(a1) < id(a2)\\n    WITH a1, a2 ORDER BY rand() LIMIT $limit\\n    CALL {\\n      WITH a1, a2\\n      SET a1._tmp = a1.name\\n      SET a1.name = a2.name\\n      SET a2.name = a1._tmp\\n      REMOVE a1._tmp\\n      SET a1:Fraudulent, a2:Fraudulent\\n      RETURN a1.name AS swapped_name1, a2.name AS swapped_name2\\n    }\\n    RETURN swapped_name1, swapped_name2\\n    '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported data into 'coauthor-constraint' (authors: 47, pairs: 120)\n",
      "Database 'coauthor-instance' cleared.\n",
      "Imported data into 'coauthor-instance' (authors: 47, pairs: 120)\n",
      "Performed 10 swaps in 'coauthor-instance'\n",
      "\n",
      "Import and swap workflow complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Verify connectivity and run import/swap workflow ---\n",
    "\n",
    "# Test connection\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    driver.verify_connectivity()\n",
    "    print(\"Connected to Neo4j database successfully.\")\n",
    "\n",
    "# Run import: constraint DB (canonical)\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    clear_database(driver, CONSTRAINT_DB)\n",
    "    setup_database(driver, CONSTRAINT_DB)\n",
    "    import_data(driver, authors, connections, CONSTRAINT_DB)\n",
    "\n",
    "# Run import: instance DB (sandbox) and perform swaps\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    clear_database(driver, INSTANCE_DB)\n",
    "    # do NOT create the unique constraint on instance DB (swaps could violate it)\n",
    "    import_data(driver, authors, connections, INSTANCE_DB)\n",
    "    swap_random_authors(driver, INSTANCE_DB, int(hypp.fraud_number))\n",
    "\n",
    "print(\"\\nImport and swap workflow complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb6ed7",
   "metadata": {},
   "source": [
    "## Validation & Next Steps\n",
    "\n",
    "- Constraint graph: clean, deduplicated author data with co-author relationships\n",
    "- Instance graph: same structure with `hypp.fraud_number` swapped identities labeled `:Fraudulent`\n",
    "\n",
    "def swap_random_authors(driver, database, number_of_swaps):\n",
    "    query = \"\"\"\n",
    "    // 1. Get a pool of authors and pick two random ones\n",
    "    MATCH (a1:Author), (a2:Author)\n",
    "    WHERE elementId(a1) < elementId(a2) // Ensure we don't pick the same node twice\n",
    "    WITH a1, a2 ORDER BY rand()\n",
    "    LIMIT $limit\n",
    "\n",
    "    // 2. Store their names in temporary variables\n",
    "    WITH a1, a2, a1.name AS oldName1, a2.name AS oldName2\n",
    "    \n",
    "    // 3. Perform the swap\n",
    "    SET a1.name = oldName2\n",
    "    SET a2.name = oldName1\n",
    "    \n",
    "    // 4. Label them as fraudulent/swapped so we can find them\n",
    "    SET a1:Fraudulent, a2:Fraudulent\n",
    "    \n",
    "    RETURN a1.name, a2.name\n",
    "    \"\"\"\n",
    "    \n",
    "    with driver.session(database=database) as session:\n",
    "        result = session.run(query, limit=number_of_swaps)\n",
    "        for record in result:\n",
    "            print(f\"Swapped identities: {record[0]} <-> {record[1]}\")\n",
    "\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    clear_database(driver, \"test-instance-graph\")\n",
    "    # setup_database(driver, \"test-instance-graph\") probably constraint not for indtance bc duplicates could exist in it\n",
    "    import_data(driver, authors, connections, \"test-instance-graph\")\n",
    "    swap_random_authors(driver, \"test-instance-graph\", hypp.fraud_number)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
