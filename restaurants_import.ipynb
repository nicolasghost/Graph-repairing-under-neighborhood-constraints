{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56fe66ac",
   "metadata": {},
   "source": [
    "# Restaurant Data Import \n",
    "\n",
    "This notebook:\n",
    "1. Loads restaurant data from the fz dataset\n",
    "2. Imports clean data into a **constraint graph** \n",
    "3. Creates an **instance graph** \n",
    "\n",
    "\n",
    "**Workflow:**\n",
    "- Environment setup → Load data → Import to Neo4j → Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff4408",
   "metadata": {},
   "source": [
    "## Step 1: Environment Configuration\n",
    "\n",
    "Load Neo4j credentials and database names from `.env` file.\n",
    "Never commit secrets — use `.env` (gitignored) for local credentials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79e1e5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Neo4j URI: 'neo4j://127.0.0.1:7687', username: 'neo4j' (password hidden)\n"
     ]
    }
   ],
   "source": [
    "# --- Env loader & sanitiser ---\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "# .env path \n",
    "env_path = pathlib.Path.cwd() / \".env\"\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "def _strip_quotes(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    return v.strip().strip('\"').strip(\"'\")\n",
    "\n",
    "# sanitized env values\n",
    "URI = _strip_quotes(os.getenv(\"NEO4J_URI\"))\n",
    "USERNAME = _strip_quotes(os.getenv(\"NEO4J_USERNAME\"))\n",
    "PASSWORD = _strip_quotes(os.getenv(\"NEO4J_PASSWORD\"))\n",
    "\n",
    "missing = [k for k,v in ((\"NEO4J_URI\",URI),(\"NEO4J_USERNAME\",USERNAME),(\"NEO4J_PASSWORD\",PASSWORD)) if not v]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing env vars: {missing}. Edit .env or export env vars and re-run this cell.\")\n",
    "\n",
    "# printing safe info only\n",
    "print(f\"Loaded Neo4j URI: {URI!r}, username: {USERNAME!r} (password hidden)\")\n",
    "AUTH = (USERNAME, PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d609bb",
   "metadata": {},
   "source": [
    "## Step 2: Configure Databases & Parameters\n",
    "\n",
    "Set constraint and instance database names, fraud count.\n",
    "All values can be overridden via `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "402aa3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint DB: restaurant-constraint\n",
      "Instance DB: restaurant-instance\n"
     ]
    }
   ],
   "source": [
    "# --- DB names, hyperparameters, and defaults ---\n",
    "CONSTRAINT_DB = _strip_quotes(os.getenv(\"NEO4J_CONSTRAINT_DB\")) or \"test\"\n",
    "INSTANCE_DB   = _strip_quotes(os.getenv(\"NEO4J_INSTANCE_DB\")) or \"test-instance-graph\"\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "hypp = edict()\n",
    "\n",
    "print(\"Constraint DB:\", CONSTRAINT_DB)\n",
    "print(\"Instance DB:\", INSTANCE_DB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235e2dc",
   "metadata": {},
   "source": [
    "## Step 3: Locate Data Files\n",
    "\n",
    "Auto-detects the latest cleaned restaurant and similarity files based on their timestampss from `datasets/temp/`.\n",
    "These are generated by `restaurant_data_cleaning.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c021f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RESTAURANTS_PATH = datasets\\temp\\restaurants_20260107-125146.txt\n",
      "Using SIMILARITIES_PATH = datasets\\temp\\restaurant_similarities_20260107-125146.txt\n"
     ]
    }
   ],
   "source": [
    "# --- Data paths (latest by timestamp) ---\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def latest_by_timestamp(folder, pattern, name_re, dt_fmt=\"%Y%m%d-%H%M%S\"):\n",
    "    folder = Path(folder)\n",
    "    candidates = list(folder.glob(pattern))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    ts_files = []\n",
    "    rx = re.compile(name_re)\n",
    "    for p in candidates:\n",
    "        m = rx.search(p.name)\n",
    "        if m:\n",
    "            try:\n",
    "                ts = datetime.strptime(m.group(1), dt_fmt)\n",
    "                ts_files.append((ts, p))\n",
    "            except Exception:\n",
    "                pass\n",
    "    if ts_files:\n",
    "        return str(max(ts_files, key=lambda t_p: t_p[0])[1])\n",
    "    return str(max(candidates, key=lambda p: p.stat().st_mtime))\n",
    "\n",
    "base = Path(\"datasets\") / \"temp\"\n",
    "RESTAURANTS_PATH = latest_by_timestamp(base, \"restaurants_*.txt\", r\"restaurants_(\\d{8}-\\d{6})\\.txt\")\n",
    "SIMILARITIES_PATH = latest_by_timestamp(base, \"restaurant_similarities_*.txt\", r\"restaurant_similarities_(\\d{8}-\\d{6})\\.txt\")\n",
    "\n",
    "print(\"Using RESTAURANTS_PATH =\", RESTAURANTS_PATH)\n",
    "print(\"Using SIMILARITIES_PATH =\", SIMILARITIES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d448c022",
   "metadata": {},
   "source": [
    "## Step 4: Load Data into Memory\n",
    "\n",
    "Read the cleaned restaurant nodes and similarity edges from the detected files into Python lists. \n",
    "This prepares the data for bulk import into Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eefe20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 864 restaurants and 5307 similarity edges.\n",
      "Sample restaurant: {'id': '0', 'name': \"arnie morton's of chicago\", 'area_code': '310', 'addr': ' \"435 s. la cienega blv.\"', 'city': ' \"los angeles\"'}\n",
      "Sample similarity edge: ('0', '1')\n"
     ]
    }
   ],
   "source": [
    "# --- Load restaurant nodes and similarity edges into memory\n",
    "# Read restaurants file\n",
    "restaurants = []\n",
    "with open(RESTAURANTS_PATH, encoding=\"utf-8\") as f:\n",
    "    next(f)  # skip header\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) >= 5:\n",
    "            # id, name, area_code, addr, city\n",
    "            restaurants.append({\n",
    "                \"id\": parts[0],\n",
    "                \"name\": parts[1],\n",
    "                \"area_code\": parts[2],\n",
    "                \"addr\": parts[3],\n",
    "                \"city\": parts[4]\n",
    "            })\n",
    "\n",
    "# Read similarity edges (expects lines like \"(id1,id2)\")\n",
    "similarities = []\n",
    "with open(SIMILARITIES_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        clean_line = line.strip().lstrip(\"(\").rstrip(\")\")\n",
    "        parts = [p.strip() for p in clean_line.split(\",\")]\n",
    "        if len(parts) == 2:\n",
    "            similarities.append((parts[0], parts[1]))\n",
    "\n",
    "print(f\"Loaded {len(restaurants)} restaurants and {len(similarities)} similarity edges.\")\n",
    "print(\"Sample restaurant:\", restaurants[0])\n",
    "print(\"Sample similarity edge:\", similarities[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014e3544",
   "metadata": {},
   "source": [
    "## Step 5: Define Neo4j Operations\n",
    "\n",
    "Helper functions:\n",
    "- `clear_database()` — remove all nodes/relationships\n",
    "- `setup_database()` — create uniqueness constraints\n",
    "- `import_data()` — bulk insert restaurant nodes and similarity edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c55f3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Neo4j helper functions and import workflow ---\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "def clear_database(driver, database):\n",
    "    driver.execute_query(\"MATCH (n) DETACH DELETE n\", database_=database)\n",
    "    print(f\"Database '{database}' cleared.\")\n",
    "\n",
    "def setup_database(driver, database):\n",
    "    # Enforce unique restaurant id \n",
    "    driver.execute_query(\"\"\"\n",
    "        CREATE CONSTRAINT restaurant_id_unique IF NOT EXISTS\n",
    "        FOR (r:Restaurant) REQUIRE r.id IS UNIQUE\n",
    "    \"\"\", database_=database)\n",
    "    print(f\"Constraint ensured on '{database}'.\")\n",
    "\n",
    "def import_data(driver, restaurants_list, similarities_list, database):\n",
    "    # Bulk create restaurant nodes\n",
    "    driver.execute_query(\"\"\"\n",
    "        UNWIND $restaurants AS r\n",
    "        MERGE (n:Restaurant {id: r.id})\n",
    "        SET n.name = r.name, n.area_code = r.area_code, n.addr = r.addr, n.city = r.city\n",
    "    \"\"\", restaurants=restaurants_list, database_=database)\n",
    "    # Bulk create similarity edges\n",
    "    driver.execute_query(\"\"\"\n",
    "        UNWIND $pairs AS pair\n",
    "        MATCH (a:Restaurant {id: pair[0]})\n",
    "        MATCH (b:Restaurant {id: pair[1]})\n",
    "        MERGE (a)-[:SIMILAR]->(b)\n",
    "    \"\"\", pairs=similarities_list, database_=database)\n",
    "    print(f\"Imported data into '{database}' (restaurants: {len(restaurants_list)}, pairs: {len(similarities_list)})\")\n",
    "\n",
    "def visualize_constraint_graph(driver, database):\n",
    "    # Add self-loop CONSTRAINT edges so the area-code rule is visible in Neo4j Browser.\n",
    "    driver.execute_query(\n",
    "        \"MATCH (:Restaurant)-[rel:CONSTRAINT]->() DELETE rel\",\n",
    "        database_=database,\n",
    "    )\n",
    "    driver.execute_query(\n",
    "        \"MATCH (r:Restaurant) MERGE (r)-[:CONSTRAINT]->(r)\",\n",
    "        database_=database,\n",
    "    )\n",
    "    print(f\"Constraint self-loops refreshed on '{database}' (visualization only).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf8011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Neo4j database successfully.\n",
      "Database 'restaurant-constraint' cleared.\n",
      "Constraint ensured on 'restaurant-constraint'.\n",
      "Imported data into 'restaurant-constraint' (restaurants: 864, pairs: 5307)\n",
      "Constraint self-loops refreshed on 'restaurant-constraint' (visualization only).\n",
      "Database 'restaurant-instance' cleared.\n",
      "Imported data into 'restaurant-instance' (restaurants: 864, pairs: 5307)\n",
      "\n",
      "Import workflow complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Verify connectivity and run import workflow ---\n",
    "# Test connection\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    driver.verify_connectivity()\n",
    "    print(\"Connected to Neo4j database successfully.\")\n",
    "\n",
    "# Run import: constraint DB (canonical)\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    clear_database(driver, CONSTRAINT_DB)\n",
    "    setup_database(driver, CONSTRAINT_DB)\n",
    "    import_data(driver, restaurants, similarities, CONSTRAINT_DB)\n",
    "    visualize_constraint_graph(driver, CONSTRAINT_DB)  # optional visualization\n",
    "\n",
    "# Run import: instance DB (sandbox)\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    clear_database(driver, INSTANCE_DB)\n",
    "    # do NOT create the unique constraint on instance DB (to allow perturbations)\n",
    "    import_data(driver, restaurants, similarities, INSTANCE_DB)\n",
    "\n",
    "print(\"\\nImport workflow complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af575bc2",
   "metadata": {},
   "source": [
    "## Validation & Next Steps\n",
    "\n",
    "\n",
    "- Constraint DB: `Restaurant` nodes with `:SIMILAR` relationships; optional `:CONSTRAINT` self-loops added for visualization.\n",
    "- Instance DB: same structure, used as sandbox for perturbation/repair experiments.\n",
    "\n",
    "**Neo4j Browser checks:**\n",
    "\n",
    "\n",
    "```cypher\n",
    "// Basic counts\n",
    "MATCH (r:Restaurant) RETURN count(r) AS total_restaurants;\n",
    "MATCH (r:Restaurant)-[:SIMILAR]->(s:Restaurant) RETURN count(*) AS similarity_edges;\n",
    "// Potential violations (should be 0 in ground truth graphs)\n",
    "MATCH (a:Restaurant)-[:SIMILAR]->(b:Restaurant)\n",
    "WHERE a.area_code <> b.area_code\n",
    "RETURN count(*) AS violations;\n",
    "```\n",
    "\n",
    "**Ground truth artifacts written:**\n",
    "- `datasets/temp/restaurants_cleaned_*.txt`\n",
    "- `datasets/temp/restaurant_similarities_cleaned_*.txt`\n",
    "\n",
    "\n",
    "Use these as inputs for downstream perturbation and repair experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f2f63",
   "metadata": {},
   "source": [
    "## Step 6: Build Constraint and Instance Graphs (NetworkX)\n",
    "\n",
    "We mirror the Cora pipeline:\n",
    "\n",
    "- Constraint graph `S`: nodes are area codes; edges are self-loops (only equal area codes allowed as neighbors).\n",
    "- Instance graph `G`: nodes are restaurants with label = `area_code`; edges from address+city similarity pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db474855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build graphs S (constraint) and G (instance) ---\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# Constraint graph: area codes as nodes, only self-loops allowed\n",
    "S = nx.Graph()\n",
    "area_codes = sorted({r[\"area_code\"] for r in restaurants if r.get(\"area_code\")})\n",
    "S.add_nodes_from(area_codes)\n",
    "S.add_edges_from((ac, ac) for ac in area_codes)\n",
    "\n",
    "# Instance graph: restaurant nodes with labels, attributes, and similarity edges\n",
    "G = nx.Graph()\n",
    "for r in restaurants:\n",
    "    rid = int(r[\"id\"])\n",
    "    G.add_node(rid)\n",
    "    G.nodes[rid][\"label\"] = r[\"area_code\"]  # label used for constraint checks\n",
    "    G.nodes[rid][\"name\"] = r[\"name\"]\n",
    "    G.nodes[rid][\"addr\"] = r[\"addr\"]\n",
    "    G.nodes[rid][\"city\"] = r[\"city\"]\n",
    "\n",
    "for i, j in similarities:\n",
    "    u, v = int(i), int(j)\n",
    "    if u == v:\n",
    "        continue\n",
    "    G.add_edge(u, v)\n",
    "\n",
    "print(f\"Constraint graph S: |L|={len(S.nodes)}, |N|={len(S.edges)} (self-loops)\")\n",
    "print(f\"Instance graph G: |V|={len(G.nodes)}, |E|={len(G.edges)}, avg deg={0 if len(G)==0 else (2*len(G.edges)/len(G)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecdefda",
   "metadata": {},
   "source": [
    "## Step 7: Clean to Ground Truth (No Violations)\n",
    "\n",
    "We iteratively remove nodes that participate in a violating edge until all remaining edges comply with the constraint graph `S` (i.e., neighbors must share the same area code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96652e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Violations and cleaning ---\n",
    "\n",
    "def violations(G_in, S_in):\n",
    "    \"\"\"Return list of violating edges (u,v) where labels of endpoints are not allowed by S.\"\"\"\n",
    "    out = []\n",
    "    for (u, v) in G_in.edges():\n",
    "        lu = G_in.nodes[u].get(\"label\")\n",
    "        lv = G_in.nodes[v].get(\"label\")\n",
    "        # allowed only if S has an edge between labels (here: self-loop equality)\n",
    "        if not S_in.has_edge(lu, lv):\n",
    "            out.append((u, v))\n",
    "    return out\n",
    "\n",
    "def clean_to_ground_truth(G_in, S_in):\n",
    "    Gc = G_in.copy()\n",
    "    removed = set()\n",
    "\n",
    "    # Remove nodes participating in violations until none remain\n",
    "    while True:\n",
    "        viols = violations(Gc, S_in)\n",
    "        if not viols:\n",
    "            break\n",
    "        u, v = viols[0]\n",
    "        # simple heuristic: remove the lower-degree endpoint\n",
    "        if Gc.degree[u] <= Gc.degree[v]:\n",
    "            drop = u\n",
    "        else:\n",
    "            drop = v\n",
    "        removed.add(drop)\n",
    "        Gc.remove_node(drop)\n",
    "    return Gc, removed\n",
    "\n",
    "G_opt, removed_nodes = clean_to_ground_truth(G, S)\n",
    "\n",
    "print(\"Cleaning complete.\")\n",
    "print(f\"Removed nodes: {len(removed_nodes)}\")\n",
    "print(f\"G_opt: |V|={len(G_opt.nodes)}, |E|={len(G_opt.edges)}, avg deg={0 if len(G_opt)==0 else (2*len(G_opt.edges)/len(G_opt)):.2f}\")\n",
    "print(f\"Violations remaining: {len(violations(G_opt, S))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b866c1",
   "metadata": {},
   "source": [
    "## Step 8: Save Cleaned Graph Files\n",
    "\n",
    "Writes ground-truth files for downstream perturbation/repair experiments:\n",
    "\n",
    "- `datasets/temp/restaurants_cleaned_*.txt` (id, name, area_code, addr, city)\n",
    "- `datasets/temp/restaurant_similarities_cleaned_*.txt` (pairs `(i,j)` for edges in `G_opt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf9102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Persist cleaned outputs ---\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Index restaurants by id for quick lookup\n",
    "by_id = {int(r[\"id\"]): r for r in restaurants}\n",
    "\n",
    "# Write cleaned restaurants (nodes remaining in G_opt)\n",
    "clean_restaurants_path = Path(\"datasets/temp\") / f\"restaurants_cleaned_{timestamp}.txt\"\n",
    "with open(clean_restaurants_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"id\\tname\\tarea_code\\taddr\\tcity\\n\")\n",
    "    for rid in sorted(G_opt.nodes):\n",
    "        r = by_id.get(rid)\n",
    "        if not r:\n",
    "            continue\n",
    "        f.write(f\"{rid}\\t{r['name']}\\t{r['area_code']}\\t{r['addr']}\\t{r['city']}\\n\")\n",
    "\n",
    "# Write cleaned similarities (edges in G_opt, undirected, write u<v once)\n",
    "clean_similarities_path = Path(\"datasets/temp\") / f\"restaurant_similarities_cleaned_{timestamp}.txt\"\n",
    "with open(clean_similarities_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for u, v in G_opt.edges():\n",
    "        a, b = (u, v) if u < v else (v, u)\n",
    "        f.write(f\"({a},{b})\\n\")\n",
    "\n",
    "print(\"Saved cleaned ground truth:\")\n",
    "print(\"  Restaurants:\", clean_restaurants_path)\n",
    "print(\"  Similarities:\", clean_similarities_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
