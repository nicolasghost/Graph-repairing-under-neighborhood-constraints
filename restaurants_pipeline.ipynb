{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb1ef0b",
   "metadata": {},
   "source": [
    "# Restaurant Graph Pipeline (fz dataset)\n",
    "\n",
    "This notebook builds the full pipeline for the restaurant (fz) dataset:\n",
    "- Clean raw ARFF and extract `area_code`\n",
    "- Construct similarity edges based on `(addr, city)`\n",
    "- Build graphs: constraint graph `S` and instance graph `G`\n",
    "- Produce cleaned ground truth `G_opt` with no violations\n",
    "- Persist artifacts for downstream perturbation/repair experiments\n",
    "\n",
    "Parameters (tunable below): address distance threshold, file paths, and optional Neo4j import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2b07841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetworkX available: True\n",
      "ARFF path: datasets\\restaurant\\fz.arff\n",
      "Output dir: datasets\\temp\n"
     ]
    }
   ],
   "source": [
    "# Setup and Parameters\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Edit distance\n",
    "try:\n",
    "    import Levenshtein\n",
    "    def string_distance(a, b):\n",
    "        return Levenshtein.distance(str(a), str(b))\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"python-Levenshtein is required; install via pyproject or pip.\")\n",
    "\n",
    "# Optional: NetworkX for convenience (graphs + stats)\n",
    "try:\n",
    "    import networkx as nx\n",
    "    NX_AVAILABLE = True\n",
    "except Exception:\n",
    "    NX_AVAILABLE = False\n",
    "\n",
    "# Parameters (tune as needed)\n",
    "ARFF_PATH = Path(\"datasets/restaurant/fz.arff\")\n",
    "OUTPUT_DIR = Path(\"datasets/temp\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ADDRESS_DISTANCE_THRESHOLD = 7  # max edit distance for addr similarity\n",
    "\n",
    "print(\"NetworkX available:\", NX_AVAILABLE)\n",
    "print(\"ARFF path:\", ARFF_PATH)\n",
    "print(\"Output dir:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "840f8a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 864 rows; columns: ['name', 'addr', 'city', 'phone', 'type', 'class']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>addr</th>\n",
       "      <th>city</th>\n",
       "      <th>phone</th>\n",
       "      <th>type</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arnie morton's of chicago</td>\n",
       "      <td>\"435 s. la cienega blv.\"</td>\n",
       "      <td>\"los angeles\"</td>\n",
       "      <td>\"310/246-1501\"</td>\n",
       "      <td>\"american\"</td>\n",
       "      <td>'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arnie morton's of chicago</td>\n",
       "      <td>\"435 s. la cienega blvd.\"</td>\n",
       "      <td>\"los angeles\"</td>\n",
       "      <td>\"310-246-1501\"</td>\n",
       "      <td>\"steakhouses\"</td>\n",
       "      <td>'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>art's delicatessen</td>\n",
       "      <td>\"12224 ventura blvd.\"</td>\n",
       "      <td>\"studio city\"</td>\n",
       "      <td>\"818/762-1221\"</td>\n",
       "      <td>\"american\"</td>\n",
       "      <td>'1'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name                        addr            city  \\\n",
       "0  arnie morton's of chicago    \"435 s. la cienega blv.\"   \"los angeles\"   \n",
       "1  arnie morton's of chicago   \"435 s. la cienega blvd.\"   \"los angeles\"   \n",
       "2         art's delicatessen       \"12224 ventura blvd.\"   \"studio city\"   \n",
       "\n",
       "             phone            type class  \n",
       "0   \"310/246-1501\"      \"american\"   '0'  \n",
       "1   \"310-246-1501\"   \"steakhouses\"   '0'  \n",
       "2   \"818/762-1221\"      \"american\"   '1'  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ARFF into DataFrame\n",
    "from io import StringIO\n",
    "\n",
    "def arff_to_dataframe(filepath: Path) -> pd.DataFrame:\n",
    "    data = False\n",
    "    header = \"\"\n",
    "    csv_content = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            if \"@attribute\" in line.lower():\n",
    "                attributes = line.split()\n",
    "                attri_idx = next(i for i, x in enumerate(attributes) if x.lower() == \"@attribute\")\n",
    "                column_name = attributes[attri_idx + 1]\n",
    "                header = header + column_name + \",\"\n",
    "            elif \"@data\" in line.lower():\n",
    "                data = True\n",
    "                header = header.rstrip(',') + '\\n'\n",
    "                csv_content.append(header)\n",
    "            elif data and line.strip():\n",
    "                csv_content.append(line + '\\n')\n",
    "    csv_string = ''.join(csv_content)\n",
    "    df_local = pd.read_csv(StringIO(csv_string), quotechar='\"')\n",
    "    return df_local\n",
    "\n",
    "# Load\n",
    "df = arff_to_dataframe(ARFF_PATH)\n",
    "print(f\"Loaded {len(df)} rows; columns: {list(df.columns)}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f9d3350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area code extraction:\n",
      "  total: 864\n",
      "  with area_code: 864\n",
      "  unique area_codes: 11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>phone</th>\n",
       "      <th>area_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arnie morton's of chicago</td>\n",
       "      <td>\"310/246-1501\"</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arnie morton's of chicago</td>\n",
       "      <td>\"310-246-1501\"</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>art's delicatessen</td>\n",
       "      <td>\"818/762-1221\"</td>\n",
       "      <td>818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>art's deli</td>\n",
       "      <td>\"818-762-1221\"</td>\n",
       "      <td>818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hotel bel-air</td>\n",
       "      <td>\"310/472-1211\"</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name            phone area_code\n",
       "0  arnie morton's of chicago   \"310/246-1501\"       310\n",
       "1  arnie morton's of chicago   \"310-246-1501\"       310\n",
       "2         art's delicatessen   \"818/762-1221\"       818\n",
       "3                 art's deli   \"818-762-1221\"       818\n",
       "4              hotel bel-air   \"310/472-1211\"       310"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract area code and basic cleaning\n",
    "\n",
    "def extract_area_code(phone):\n",
    "    if pd.isna(phone) or phone is None:\n",
    "        return None\n",
    "    s = str(phone).strip().strip('\"').strip(\"'\")\n",
    "    digits = s.replace('-', '').replace('/', '').replace(' ', '')\n",
    "    if len(digits) >= 3 and digits[:3].isdigit():\n",
    "        return digits[:3]\n",
    "    return None\n",
    "\n",
    "# Ensure expected columns exist\n",
    "expected_cols = {\"name\", \"phone\", \"addr\", \"city\"}\n",
    "missing = expected_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "# Compute area_code\n",
    "df[\"area_code\"] = df[\"phone\"].apply(extract_area_code)\n",
    "\n",
    "print(\"Area code extraction:\")\n",
    "print(\"  total:\", len(df))\n",
    "print(\"  with area_code:\", df[\"area_code\"].notna().sum())\n",
    "print(\"  unique area_codes:\", df[\"area_code\"].nunique())\n",
    "\n",
    "df[[\"name\", \"phone\", \"area_code\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ceed2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building similarity pairs over 864 restaurants...\n",
      "Similarity summary:\n",
      "  total edges: 5307\n",
      "  avg degree: 12.284722222222221\n",
      "Sample edges: [(0, 1), (0, 22), (0, 242), (1, 22), (1, 242), (2, 3), (2, 37), (2, 706), (3, 37), (3, 706)]\n"
     ]
    }
   ],
   "source": [
    "# Build similarity pairs based on (addr, city)\n",
    "\n",
    "similarity_edges = []\n",
    "N = len(df)\n",
    "print(f\"Building similarity pairs over {N} restaurants...\")\n",
    "for i in range(N):\n",
    "    ai = df.iloc[i]\n",
    "    for j in range(i+1, N):\n",
    "        aj = df.iloc[j]\n",
    "        if ai[\"city\"] != aj[\"city\"]:\n",
    "            continue\n",
    "        d = string_distance(ai[\"addr\"], aj[\"addr\"])\n",
    "        if d < ADDRESS_DISTANCE_THRESHOLD:\n",
    "            similarity_edges.append((i, j))\n",
    "\n",
    "print(\"Similarity summary:\")\n",
    "print(\"  total edges:\", len(similarity_edges))\n",
    "if N:\n",
    "    print(\"  avg degree:\", (2*len(similarity_edges))/N)\n",
    "print(\"Sample edges:\", similarity_edges[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f23b161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "   datasets\\temp\\restaurants_20260120-131834.txt\n",
      "   datasets\\temp\\restaurant_similarities_20260120-131834.txt\n"
     ]
    }
   ],
   "source": [
    "# Persist cleaned temp outputs (raw graph inputs)\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "restaurants_path = OUTPUT_DIR / f\"restaurants_{ts}.txt\"\n",
    "similarities_path = OUTPUT_DIR / f\"restaurant_similarities_{ts}.txt\"\n",
    "\n",
    "# Ensure area_code exists to avoid KeyError if earlier cell wasn't run\n",
    "if \"area_code\" not in df.columns:\n",
    "    def _extract_area_code_local(phone):\n",
    "        if pd.isna(phone) or phone is None:\n",
    "            return None\n",
    "        s = str(phone).strip().strip('\"').strip(\"'\")\n",
    "        digits = s.replace('-', '').replace('/', '').replace(' ', '')\n",
    "        if len(digits) >= 3 and digits[:3].isdigit():\n",
    "            return digits[:3]\n",
    "        return None\n",
    "    df[\"area_code\"] = df[\"phone\"].apply(_extract_area_code_local)\n",
    "\n",
    "with open(restaurants_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"id\\tname\\tarea_code\\taddr\\tcity\\n\")\n",
    "    for idx, row in df.iterrows():\n",
    "        f.write(f\"{idx}\\t{row['name']}\\t{row['area_code']}\\t{row['addr']}\\t{row['city']}\\n\")\n",
    "\n",
    "with open(similarities_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, j in similarity_edges:\n",
    "        f.write(f\"({i},{j})\\n\")\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"  \", restaurants_path)\n",
    "print(\"  \", similarities_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4593d86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph stats (before cleaning):\n",
      "  |V|=864, |E|=5307\n",
      "Cleaning results:\n",
      "  violations before: 32\n",
      "  violations after: 0\n",
      "  removed nodes: 15\n",
      "  G_opt |V|=849, |E|=5245\n"
     ]
    }
   ],
   "source": [
    "# Build graphs (S, G) and clean to ground truth (G_opt)\n",
    "\n",
    "# Constraint graph S: nodes = area codes; only self-loops allowed\n",
    "labels = sorted(df[\"area_code\"].dropna().unique().tolist())\n",
    "\n",
    "if NX_AVAILABLE:\n",
    "    S = nx.Graph()\n",
    "    S.add_nodes_from(labels)\n",
    "    S.add_edges_from((ac, ac) for ac in labels)\n",
    "else:\n",
    "    S = {ac: {ac} for ac in labels}  # adjacency-by-label for checks\n",
    "\n",
    "# Instance graph G: nodes = restaurant indices, label = area_code, edges from similarity pairs\n",
    "if NX_AVAILABLE:\n",
    "    G = nx.Graph()\n",
    "    for idx, row in df.iterrows():\n",
    "        G.add_node(int(idx))\n",
    "        G.nodes[int(idx)][\"label\"] = row[\"area_code\"]\n",
    "        G.nodes[int(idx)][\"name\"] = row[\"name\"]\n",
    "        G.nodes[int(idx)][\"addr\"] = row[\"addr\"]\n",
    "        G.nodes[int(idx)][\"city\"] = row[\"city\"]\n",
    "    for u, v in similarity_edges:\n",
    "        if u != v:\n",
    "            G.add_edge(int(u), int(v))\n",
    "else:\n",
    "    # Lightweight structure without networkx\n",
    "    G_nodes = {int(idx): {\n",
    "        \"label\": row[\"area_code\"],\n",
    "        \"name\": row[\"name\"],\n",
    "        \"addr\": row[\"addr\"],\n",
    "        \"city\": row[\"city\"],\n",
    "    } for idx, row in df.iterrows()}\n",
    "    G_edges = {(min(int(u), int(v)), max(int(u), int(v))) for (u, v) in similarity_edges if u != v}\n",
    "\n",
    "# Violation check: neighbors must share same area_code\n",
    "\n",
    "def has_edge_in_S(lu, lv):\n",
    "    if NX_AVAILABLE:\n",
    "        return S.has_edge(lu, lv)\n",
    "    return lv in S.get(lu, set())\n",
    "\n",
    "# Helpers to get the FIRST violating edge with respect to the CURRENT graph state\n",
    "if NX_AVAILABLE:\n",
    "    def first_violation(Gx):\n",
    "        for (u, v) in Gx.edges():\n",
    "            lu = Gx.nodes[u].get(\"label\")\n",
    "            lv = Gx.nodes[v].get(\"label\")\n",
    "            if not has_edge_in_S(lu, lv):\n",
    "                return (u, v)\n",
    "        return None\n",
    "\n",
    "    def count_violations(Gx):\n",
    "        c = 0\n",
    "        for (u, v) in Gx.edges():\n",
    "            lu = Gx.nodes[u].get(\"label\")\n",
    "            lv = Gx.nodes[v].get(\"label\")\n",
    "            if not has_edge_in_S(lu, lv):\n",
    "                c += 1\n",
    "        return c\n",
    "else:\n",
    "    def first_violation(_unused):\n",
    "        for (u, v) in G_edges:\n",
    "            lu = G_nodes[u][\"label\"]\n",
    "            lv = G_nodes[v][\"label\"]\n",
    "            if not has_edge_in_S(lu, lv):\n",
    "                return (u, v)\n",
    "        return None\n",
    "\n",
    "    def count_violations_non_nx():\n",
    "        c = 0\n",
    "        for (u, v) in G_edges:\n",
    "            lu = G_nodes[u][\"label\"]\n",
    "            lv = G_nodes[v][\"label\"]\n",
    "            if not has_edge_in_S(lu, lv):\n",
    "                c += 1\n",
    "        return c\n",
    "\n",
    "# Cleaning: iteratively remove the lower-degree endpoint of first violation until none remain\n",
    "removed = set()\n",
    "if NX_AVAILABLE:\n",
    "    G_opt = G.copy()\n",
    "    viol_before = count_violations(G_opt)\n",
    "    while True:\n",
    "        pair = first_violation(G_opt)\n",
    "        if not pair:\n",
    "            break\n",
    "        u, v = pair\n",
    "        # Guard in case of transient references\n",
    "        if u not in G_opt or v not in G_opt:\n",
    "            continue\n",
    "        drop = u if G_opt.degree[u] <= G_opt.degree[v] else v\n",
    "        removed.add(drop)\n",
    "        G_opt.remove_node(drop)\n",
    "    viol_after = count_violations(G_opt)\n",
    "else:\n",
    "    # Non-NX cleaning\n",
    "    viol_before = count_violations_non_nx()\n",
    "    remaining_nodes = set(G_nodes.keys())\n",
    "    while True:\n",
    "        pair = first_violation(None)\n",
    "        if not pair:\n",
    "            break\n",
    "        u, v = pair\n",
    "        deg_u = sum(1 for e in G_edges if u in e)\n",
    "        deg_v = sum(1 for e in G_edges if v in e)\n",
    "        drop = u if deg_u <= deg_v else v\n",
    "        removed.add(drop)\n",
    "        remaining_nodes.discard(drop)\n",
    "        G_edges = {e for e in G_edges if drop not in e}\n",
    "    viol_after = count_violations_non_nx()\n",
    "\n",
    "print(\"Graph stats (before cleaning):\")\n",
    "if NX_AVAILABLE:\n",
    "    print(f\"  |V|={len(G.nodes)}, |E|={len(G.edges)}\")\n",
    "else:\n",
    "    print(f\"  |V|={len(G_nodes)}, |E|={len(G_edges)}\")\n",
    "\n",
    "print(\"Cleaning results:\")\n",
    "print(\"  violations before:\", viol_before)\n",
    "print(\"  violations after:\", viol_after)\n",
    "print(\"  removed nodes:\", len(removed))\n",
    "if NX_AVAILABLE:\n",
    "    print(f\"  G_opt |V|={len(G_opt.nodes)}, |E|={len(G_opt.edges)}\")\n",
    "else:\n",
    "    print(f\"  G_opt |V|={len(remaining_nodes)}, |E|={len(G_edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5125638d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned ground truth:\n",
      "   datasets\\temp\\restaurants_cleaned_20260120-130842.txt\n",
      "   datasets\\temp\\restaurant_similarities_cleaned_20260120-130842.txt\n"
     ]
    }
   ],
   "source": [
    "# Persist cleaned ground truth artifacts\n",
    "\n",
    "ts2 = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "clean_restaurants_path = OUTPUT_DIR / f\"restaurants_cleaned_{ts2}.txt\"\n",
    "clean_similarities_path = OUTPUT_DIR / f\"restaurant_similarities_cleaned_{ts2}.txt\"\n",
    "\n",
    "# Helper to iterate nodes/edges in G_opt independent of NetworkX\n",
    "if NX_AVAILABLE:\n",
    "    nodes_iter = sorted(G_opt.nodes)\n",
    "    edges_iter = sorted((min(u, v), max(u, v)) for (u, v) in G_opt.edges())\n",
    "else:\n",
    "    nodes_iter = sorted(remaining_nodes)\n",
    "    edges_iter = sorted(G_edges)\n",
    "\n",
    "# Map from id to row for writing attributes\n",
    "by_id = {int(idx): row for idx, row in df.iterrows()}\n",
    "\n",
    "with open(clean_restaurants_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"id\\tname\\tarea_code\\taddr\\tcity\\n\")\n",
    "    for rid in nodes_iter:\n",
    "        r = by_id.get(int(rid))\n",
    "        if r is None:\n",
    "            continue\n",
    "        f.write(f\"{rid}\\t{r['name']}\\t{r['area_code']}\\t{r['addr']}\\t{r['city']}\\n\")\n",
    "\n",
    "with open(clean_similarities_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for u, v in edges_iter:\n",
    "        f.write(f\"({u},{v})\\n\")\n",
    "\n",
    "print(\"Saved cleaned ground truth:\")\n",
    "print(\"  \", clean_restaurants_path)\n",
    "print(\"  \", clean_similarities_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3b1f00",
   "metadata": {},
   "source": [
    "## Import into Neo4j\n",
    "\n",
    "If you have a local Neo4j, set credentials in `.env` and run the next cell to import the raw graph to two DBs:\n",
    "- Constraint DB (with uniqueness constraint and optional visualization self-loops)\n",
    "- Instance DB (no uniqueness constraint â€” useful for injecting perturbations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bcfdec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j URI: neo4j://127.0.0.1:7687\n",
      "Connected to Neo4j.\n",
      "Import complete.\n"
     ]
    }
   ],
   "source": [
    "# Neo4j helpers (optional)\n",
    "\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    from neo4j import GraphDatabase\n",
    "    NEO4J_AVAILABLE = True\n",
    "except Exception:\n",
    "    NEO4J_AVAILABLE = False\n",
    "\n",
    "if not NEO4J_AVAILABLE:\n",
    "    print(\"neo4j driver not available; skip this section or install 'neo4j'.\")\n",
    "else:\n",
    "    env_path = pathlib.Path.cwd() / \".env\"\n",
    "    load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "    def _strip_quotes(v):\n",
    "        return None if v is None else v.strip().strip('\"').strip(\"'\")\n",
    "\n",
    "    URI = _strip_quotes(os.getenv(\"NEO4J_URI\"))\n",
    "    USERNAME = _strip_quotes(os.getenv(\"NEO4J_USERNAME\"))\n",
    "    PASSWORD = _strip_quotes(os.getenv(\"NEO4J_PASSWORD\"))\n",
    "    CONSTRAINT_DB = _strip_quotes(os.getenv(\"NEO4J_CONSTRAINT_DB\")) or \"restaurants-constraint\"\n",
    "    INSTANCE_DB = _strip_quotes(os.getenv(\"NEO4J_INSTANCE_DB\")) or \"restaurants-instance\"\n",
    "\n",
    "    print(\"Neo4j URI:\", URI)\n",
    "    AUTH = (USERNAME, PASSWORD)\n",
    "\n",
    "    def clear_database(driver, database):\n",
    "        driver.execute_query(\"MATCH (n) DETACH DELETE n\", database_=database)\n",
    "\n",
    "    def setup_database(driver, database):\n",
    "        driver.execute_query(\n",
    "            \"\"\"\n",
    "            CREATE CONSTRAINT restaurant_id_unique IF NOT EXISTS\n",
    "            FOR (r:Restaurant) REQUIRE r.id IS UNIQUE\n",
    "            \"\"\",\n",
    "            database_=database,\n",
    "        )\n",
    "\n",
    "    def import_data(driver, restaurants_list, similarities_list, database):\n",
    "        driver.execute_query(\n",
    "            \"\"\"\n",
    "            UNWIND $restaurants AS r\n",
    "            MERGE (n:Restaurant {id: r.id})\n",
    "            SET n.name = r.name, n.area_code = r.area_code, n.addr = r.addr, n.city = r.city\n",
    "            \"\"\",\n",
    "            restaurants=restaurants_list,\n",
    "            database_=database,\n",
    "        )\n",
    "        driver.execute_query(\n",
    "            \"\"\"\n",
    "            UNWIND $pairs AS pair\n",
    "            MATCH (a:Restaurant {id: pair[0]})\n",
    "            MATCH (b:Restaurant {id: pair[1]})\n",
    "            MERGE (a)-[:SIMILAR]->(b)\n",
    "            \"\"\",\n",
    "            pairs=similarities_list,\n",
    "            database_=database,\n",
    "        )\n",
    "\n",
    "    def visualize_constraint_graph(driver, database):\n",
    "        driver.execute_query(\n",
    "            \"MATCH (:Restaurant)-[rel:CONSTRAINT]->() DELETE rel\",\n",
    "            database_=database,\n",
    "        )\n",
    "        driver.execute_query(\n",
    "            \"MATCH (r:Restaurant) MERGE (r)-[:CONSTRAINT]->(r)\",\n",
    "            database_=database,\n",
    "        )\n",
    "\n",
    "    # Prepare payloads from current in-memory data\n",
    "    restaurants_payload = []\n",
    "    for idx, row in df.iterrows():\n",
    "        restaurants_payload.append({\n",
    "            \"id\": str(idx),\n",
    "            \"name\": row[\"name\"],\n",
    "            \"area_code\": row[\"area_code\"],\n",
    "            \"addr\": row[\"addr\"],\n",
    "            \"city\": row[\"city\"],\n",
    "        })\n",
    "    similarities_payload = [(str(u), str(v)) for (u, v) in similarity_edges]\n",
    "\n",
    "    with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "        driver.verify_connectivity()\n",
    "        print(\"Connected to Neo4j.\")\n",
    "\n",
    "        # Constraint DB (canonical)\n",
    "        clear_database(driver, CONSTRAINT_DB)\n",
    "        setup_database(driver, CONSTRAINT_DB)\n",
    "        import_data(driver, restaurants_payload, similarities_payload, CONSTRAINT_DB)\n",
    "        visualize_constraint_graph(driver, CONSTRAINT_DB)\n",
    "\n",
    "        # Instance DB (sandbox)\n",
    "        clear_database(driver, INSTANCE_DB)\n",
    "        import_data(driver, restaurants_payload, similarities_payload, INSTANCE_DB)\n",
    "\n",
    "    print(\"Import complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01bc469",
   "metadata": {},
   "source": [
    "## Validation & Next Steps\n",
    "\n",
    "- Files produced for experiments:\n",
    "  - `datasets/temp/restaurants_*.txt`\n",
    "  - `datasets/temp/restaurant_similarities_*.txt`\n",
    "  - `datasets/temp/restaurants_cleaned_*.txt`\n",
    "  - `datasets/temp/restaurant_similarities_cleaned_*.txt`\n",
    "\n",
    "- Neo4j checks (Browser):\n",
    "```cypher\n",
    "// Counts\n",
    "MATCH (r:Restaurant) RETURN count(r) AS restaurants;\n",
    "MATCH (:Restaurant)-[:SIMILAR]->(:Restaurant) RETURN count(*) AS edges;\n",
    "\n",
    "// NULL-aware violations (use in both instance and constraint DBs)\n",
    "MATCH (a:Restaurant)-[:SIMILAR]->(b:Restaurant)\n",
    "WHERE a.area_code IS NULL OR b.area_code IS NULL OR a.area_code <> b.area_code\n",
    "RETURN count(*) AS violations;\n",
    "```\n",
    "\n",
    "- Notes:\n",
    "  - If you cleaned only the instance DB, query that DB in Neo4j Desktop.\n",
    "  - To make the canonical DB (constraint) the ground truth, set `TARGET_DB = CONSTRAINT_DB` in Cell 14 and rerun.\n",
    "\n",
    "- Next: add perturbation generators and repair strategies using `G_opt` as ground truth.\n",
    "- Threshold tuning: adjust `ADDRESS_DISTANCE_THRESHOLD` to control graph density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb771fe",
   "metadata": {},
   "source": [
    "## Neo4j-only Path (no NetworkX)\n",
    "\n",
    "This section performs similarity edge creation (optional, via APOC) and the iterative cleaning to ground truth entirely in Neo4j using Cypher. The Python code here only orchestrates queries; no NetworkX is used.\n",
    "\n",
    "- Build `:SIMILAR` edges in DB using APOC Levenshtein (optional)\n",
    "- Iteratively remove a low-degree endpoint from any cross-area-code edge until none remain\n",
    "- Export cleaned nodes/edges to datasets/temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5861087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APOC-based edge creation failed. Ensure APOC is installed and enabled.\n",
      "Error: {neo4j_code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `apoc.periodic.iterate` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.} {gql_status: 42001} {gql_status_description: error: syntax error or access rule violation - invalid syntax}\n",
      "You can skip this step if edges were already imported earlier.\n"
     ]
    }
   ],
   "source": [
    "# Build SIMILAR edges in Neo4j with APOC (optional)\n",
    "# Requires: Restaurants already imported (see Neo4j helpers above)\n",
    "\n",
    "try:\n",
    "    from neo4j import GraphDatabase\n",
    "    NEO4J_AVAILABLE = True\n",
    "except Exception:\n",
    "    NEO4J_AVAILABLE = False\n",
    "\n",
    "if not NEO4J_AVAILABLE:\n",
    "    print(\"neo4j driver not available; skip this section or install 'neo4j'.\")\n",
    "else:\n",
    "    import pathlib\n",
    "    from dotenv import load_dotenv\n",
    "    env_path = pathlib.Path.cwd() / \".env\"\n",
    "    load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "    def _strip_quotes(v):\n",
    "        return None if v is None else v.strip().strip('\"').strip(\"'\")\n",
    "\n",
    "    URI = _strip_quotes(os.getenv(\"NEO4J_URI\"))\n",
    "    USERNAME = _strip_quotes(os.getenv(\"NEO4J_USERNAME\"))\n",
    "    PASSWORD = _strip_quotes(os.getenv(\"NEO4J_PASSWORD\"))\n",
    "    INSTANCE_DB = _strip_quotes(os.getenv(\"NEO4J_INSTANCE_DB\")) or \"restaurants-instance\"\n",
    "\n",
    "    threshold = ADDRESS_DISTANCE_THRESHOLD\n",
    "\n",
    "    apoc_stmt = (\n",
    "        \"CALL apoc.periodic.iterate(\\n\"\n",
    "        \"  'MATCH (a:Restaurant) RETURN a',\\n\"\n",
    "        \"  'MATCH (b:Restaurant)\\\\n\"\n",
    "        \"   WHERE a.id < b.id AND a.city = b.city\\\\n\"\n",
    "        \"     AND apoc.text.levenshteinDistance(a.addr, b.addr) < $threshold\\\\n\"\n",
    "        \"   MERGE (a)-[:SIMILAR]->(b)',\\n\"\n",
    "        \"  {batchSize:1000, parallel:false, params:{threshold:$threshold}}\\n\"\n",
    "        \")\"\n",
    "    )\n",
    "\n",
    "    with GraphDatabase.driver(URI, auth=(USERNAME, PASSWORD)) as driver:\n",
    "        driver.verify_connectivity()\n",
    "        try:\n",
    "            driver.execute_query(apoc_stmt, threshold=threshold, database_=INSTANCE_DB)\n",
    "            print(f\"SIMILAR edges built in database '{INSTANCE_DB}' via APOC (threshold={threshold}).\")\n",
    "        except Exception as e:\n",
    "            print(\"APOC-based edge creation failed. Ensure APOC is installed and enabled.\")\n",
    "            print(\"Error:\", e)\n",
    "            print(\"You can skip this step if edges were already imported earlier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d97170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean to ground truth entirely in Neo4j (iterative Cypher)\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = pathlib.Path.cwd() / \".env\"\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "def _strip_quotes(v):\n",
    "    return None if v is None else v.strip().strip('\"').strip(\"'\")\n",
    "\n",
    "URI = _strip_quotes(os.getenv(\"NEO4J_URI\"))\n",
    "USERNAME = _strip_quotes(os.getenv(\"NEO4J_USERNAME\"))\n",
    "PASSWORD = _strip_quotes(os.getenv(\"NEO4J_PASSWORD\"))\n",
    "INSTANCE_DB = _strip_quotes(os.getenv(\"NEO4J_INSTANCE_DB\")) or \"restaurants-instance\"\n",
    "CONSTRAINT_DB = _strip_quotes(os.getenv(\"NEO4J_CONSTRAINT_DB\")) or \"restaurants-constraint\"\n",
    "\n",
    "# Choose which DB to clean: ground-truth usually lives in CONSTRAINT_DB.\n",
    "# Set TARGET_DB to CONSTRAINT_DB if you want the canonical DB cleaned.\n",
    "TARGET_DB = CONSTRAINT_DB  # default to canonical DB for cleaning\n",
    "\n",
    "with GraphDatabase.driver(URI, auth=(USERNAME, PASSWORD)) as driver:\n",
    "    driver.verify_connectivity()\n",
    "\n",
    "    def get_one_violation(tx):\n",
    "        # NULL-aware violation selection: any NULL or unequal area_code constitutes a violation\n",
    "        res = tx.run(\n",
    "            \"\"\"\n",
    "            MATCH (a:Restaurant)-[:SIMILAR]->(b:Restaurant)\n",
    "            WHERE a.area_code IS NULL OR b.area_code IS NULL OR a.area_code <> b.area_code\n",
    "            RETURN a.id AS u, b.id AS v\n",
    "            LIMIT 1\n",
    "            \"\"\"\n",
    "        ).data()\n",
    "        return res[0] if res else None\n",
    "\n",
    "    def degree_of(tx, rid):\n",
    "        res = tx.run(\n",
    "            \"\"\"\n",
    "            MATCH (r:Restaurant {id:$id})\n",
    "            RETURN size((r)--()) AS d\n",
    "            \"\"\",\n",
    "            id=rid,\n",
    "        ).data()\n",
    "        return (res[0][\"d\"] if res else 0)\n",
    "\n",
    "    def delete_node(tx, rid):\n",
    "        tx.run(\"MATCH (r:Restaurant {id:$id}) DETACH DELETE r\", id=rid)\n",
    "\n",
    "    removed = 0\n",
    "    with driver.session(database=TARGET_DB) as session:\n",
    "        print(f\"Cleaning Neo4j DB '{TARGET_DB}' to ground truth...\")\n",
    "        while True:\n",
    "            vpair = session.execute_read(get_one_violation)\n",
    "            if not vpair:\n",
    "                break\n",
    "            u, v = vpair[\"u\"], vpair[\"v\"]\n",
    "            du = session.execute_read(degree_of, u)\n",
    "            dv = session.execute_read(degree_of, v)\n",
    "            drop = u if du <= dv else v\n",
    "            session.execute_write(delete_node, drop)\n",
    "            removed += 1\n",
    "\n",
    "    print(f\"Neo4j cleaning complete on '{TARGET_DB}'. Removed {removed} nodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8aa2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned graph from Neo4j to files (no NetworkX)\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from neo4j import GraphDatabase\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = pathlib.Path.cwd() / \".env\"\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "def _strip_quotes(v):\n",
    "    return None if v is None else v.strip().strip('\"').strip(\"'\")\n",
    "\n",
    "URI = _strip_quotes(os.getenv(\"NEO4J_URI\"))\n",
    "USERNAME = _strip_quotes(os.getenv(\"NEO4J_USERNAME\"))\n",
    "PASSWORD = _strip_quotes(os.getenv(\"NEO4J_PASSWORD\"))\n",
    "INSTANCE_DB = _strip_quotes(os.getenv(\"NEO4J_INSTANCE_DB\")) or \"restaurants-instance\"\n",
    "\n",
    "out_ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "out_nodes = OUTPUT_DIR / f\"restaurants_cleaned_neo4j_{out_ts}.txt\"\n",
    "out_edges = OUTPUT_DIR / f\"restaurant_similarities_cleaned_neo4j_{out_ts}.txt\"\n",
    "\n",
    "with GraphDatabase.driver(URI, auth=(USERNAME, PASSWORD)) as driver:\n",
    "    driver.verify_connectivity()\n",
    "    with driver.session(database=INSTANCE_DB) as session:\n",
    "        # Nodes\n",
    "        recs = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (r:Restaurant)\n",
    "            RETURN r.id AS id, r.name AS name, r.area_code AS area_code, r.addr AS addr, r.city AS city\n",
    "            ORDER BY r.id\n",
    "            \"\"\"\n",
    "        ).data()\n",
    "        with open(out_nodes, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"id\\tname\\tarea_code\\taddr\\tcity\\n\")\n",
    "            for row in recs:\n",
    "                f.write(f\"{row['id']}\\t{row['name']}\\t{row['area_code']}\\t{row['addr']}\\t{row['city']}\\n\")\n",
    "\n",
    "        # Undirected edges once (a.id < b.id)\n",
    "        e_recs = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (a:Restaurant)-[:SIMILAR]->(b:Restaurant)\n",
    "            WHERE a.id < b.id\n",
    "            RETURN a.id AS u, b.id AS v\n",
    "            ORDER BY u, v\n",
    "            \"\"\"\n",
    "        ).data()\n",
    "        with open(out_edges, \"w\", encoding=\"utf-8\") as f:\n",
    "            for row in e_recs:\n",
    "                f.write(f\"({row['u']},{row['v']})\\n\")\n",
    "\n",
    "print(\"Exported cleaned Neo4j graph:\")\n",
    "print(\"  \", out_nodes)\n",
    "print(\"  \", out_edges)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
